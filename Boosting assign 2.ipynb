{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf634137-da79-4511-8ce3-24cd28e44e9a",
   "metadata": {},
   "source": [
    "# **ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c653129-835a-4d63-8875-19e78917e04e",
   "metadata": {},
   "source": [
    "**Q1. What is Gradient Boosting Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a819c-058e-4b2b-9fdc-ce911078f6da",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for both classification and regression problems. It is an ensemble learning method that combines the predictions of several weak learners (typically decision trees) to create a strong predictive model. The term \"gradient boosting\" refers to the optimization process used to minimize the errors of the model.\n",
    "\n",
    "Here's a general overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Base Learners (Weak Models):** Gradient Boosting builds an ensemble of weak learners, often decision trees, where each tree tries to correct the errors made by the previous ones.\n",
    "\n",
    "2. **Initialization:** The algorithm starts with a simple model, usually the mean or median of the target variable for regression problems. This initial model serves as the first approximation.\n",
    "\n",
    "3. **Sequential Training:** Subsequent models are trained sequentially, with each one focusing on reducing the errors of the combined ensemble of models generated so far.\n",
    "\n",
    "4. **Gradient Descent Optimization:** The key idea is to fit each new model to the residual errors (the differences between the actual and predicted values) of the combined ensemble. This is done by using gradient descent to find the direction and magnitude of the adjustments needed.\n",
    "\n",
    "5. **Shrinkage (or Learning Rate):** A shrinkage parameter is introduced to control the contribution of each weak learner to the ensemble. A smaller shrinkage value requires a higher number of weak learners but can lead to a more robust model.\n",
    "\n",
    "6. **Combining Weak Models:** The final prediction is the sum of the predictions from all the weak learners, each multiplied by its associated shrinkage factor.\n",
    "\n",
    "Gradient Boosting Regression is known for its high predictive accuracy and robustness against overfitting. Popular implementations include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor. These algorithms have been widely used in various applications, including finance, healthcare, and online advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7e6e3-8511-4cbf-8920-edd3457126aa",
   "metadata": {},
   "source": [
    "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12abc68d-6d1f-41cd-abc4-2b022962dc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3.8057\n",
      "R-squared: -9.1629\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean for regression\n",
    "        initial_prediction = np.mean(y)\n",
    "        prediction = np.full_like(y, initial_prediction, dtype=np.float64)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - prediction\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=3)\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Make predictions with the weak learner\n",
    "            weak_predictions = model.predict(X)\n",
    "\n",
    "            # Update the overall prediction with a fraction of the weak learner's predictions\n",
    "            prediction += self.learning_rate * weak_predictions\n",
    "\n",
    "            # Store the weak learner in the ensemble\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by combining the predictions of all weak learners\n",
    "        predictions = np.sum(self.learning_rate * model.predict(X) for model in self.models)\n",
    "        return predictions\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + 1 + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40f6f3-9862-47f0-9876-d259de532028",
   "metadata": {},
   "source": [
    "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a288041e-a148-4fd5-b261-ccd8165c5a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Mean Squared Error (Best Model): 0.0071\n",
      "R-squared (Best Model): 0.9811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_regressor,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_mean_squared_error',  # Use the negative mean squared error as it is a minimization problem\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    cv=5,  # Number of cross-validation folds\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_gb_regressor = random_search.best_estimator_\n",
    "y_pred_best = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Mean Squared Error (Best Model): {mse_best:.4f}\")\n",
    "print(f\"R-squared (Best Model): {r2_best:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e828764-325a-4b14-ae34-c3e1abe03ed0",
   "metadata": {},
   "source": [
    "**Q4. What is a weak learner in Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d00a3-f3b2-4885-b860-175149bdd1f6",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a base model that performs slightly better than random chance on a given task. Typically, decision trees are used as weak learners in Gradient Boosting algorithms, but other algorithms can also be employed.\n",
    "\n",
    "The term \"weak\" is relative, and it implies that the individual models in the ensemble are not highly accurate on their own. The strength of Gradient Boosting comes from the combination of these weak learners to form a robust and accurate predictive model.\n",
    "\n",
    "In the training process of Gradient Boosting:\n",
    "\n",
    "1. **Initialization:** The algorithm starts with a simple model, often the mean or median of the target variable in the case of regression, or a class distribution in the case of classification.\n",
    "\n",
    "2. **Sequential Training:** Subsequent weak learners are added to the ensemble in a sequential manner. Each new weak learner focuses on capturing the errors made by the combined ensemble of models generated so far.\n",
    "\n",
    "3. **Gradient Descent Optimization:** The weak learner is trained to fit the negative gradient of the loss function with respect to the ensemble's current predictions. This is done to reduce the residuals or errors of the combined model.\n",
    "\n",
    "The concept of using weak learners is fundamental to Gradient Boosting's success. Each weak learner contributes a small piece of the overall solution, and by combining them sequentially, the algorithm is able to adapt and improve its predictions over iterations. The key is to use models that are just complex enough to capture the patterns in the data but not so complex that they overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be686ff6-b133-42a5-9ae4-e15f76daa301",
   "metadata": {},
   "source": [
    "**Q5. What is the intuition behind the Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cfb06-9bc0-4ec2-a599-96a30a7611d6",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\n",
    "\n",
    "1. **Ensemble Learning:** Gradient Boosting belongs to the family of ensemble learning methods. Ensemble learning involves combining the predictions of multiple models to create a stronger and more robust model than any of its individual components.\n",
    "\n",
    "2. **Sequential Improvement:** Gradient Boosting builds an ensemble of weak learners sequentially. Each weak learner is trained to correct the errors made by the combined ensemble of models generated so far.\n",
    "\n",
    "3. **Gradient Descent Optimization:** The term \"gradient\" in Gradient Boosting refers to the use of gradient descent optimization to minimize the errors of the model. In each iteration, the algorithm identifies the direction and magnitude in the feature space where the model's predictions need improvement the most.\n",
    "\n",
    "4. **Weak Learners:** The individual models in Gradient Boosting are weak learners, meaning they are models that perform slightly better than random chance. Decision trees are commonly used as weak learners, and they are added to the ensemble in a stepwise manner.\n",
    "\n",
    "5. **Residual Fitting:** In each iteration, a new weak learner is trained to fit the residuals or errors of the current ensemble. This process allows the model to gradually reduce the errors made by the combined set of weak learners.\n",
    "\n",
    "6. **Shrinkage (Learning Rate):** A shrinkage parameter, often referred to as the learning rate, is introduced to control the contribution of each weak learner to the ensemble. A smaller learning rate requires more weak learners but can lead to a more robust and generalized model.\n",
    "\n",
    "7. **Adaptive Learning:** The algorithm is adaptive in nature. As weak learners are added sequentially, the model adapts and becomes more tailored to the specific patterns in the data, capturing complex relationships and interactions.\n",
    "\n",
    "8. **Regularization:** Gradient Boosting inherently provides a form of regularization by building models sequentially and focusing on reducing the errors made by the ensemble. This helps prevent overfitting and contributes to the model's generalization performance.\n",
    "\n",
    "In summary, the intuition behind Gradient Boosting is to iteratively improve the model's predictions by learning from the mistakes of the previous models. By combining weak learners and using gradient descent optimization, the algorithm adapts to the data's complexity and yields a powerful predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12c56b-021d-4103-96f5-960009ed630d",
   "metadata": {},
   "source": [
    "**Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355e4fa-60e8-48c7-95ce-a54199abf5d8",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. The process involves the following steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with an initial prediction, often the mean or median of the target variable in the case of regression, or a class distribution in the case of classification.\n",
    "   - This initial prediction serves as the baseline for the ensemble.\n",
    "\n",
    "2. **Residual Calculation:**\n",
    "   - The difference between the actual target values and the current prediction (residuals) is calculated for each data point in the training set.\n",
    "\n",
    "3. **Sequential Training:**\n",
    "   - A weak learner (typically a decision tree) is trained on the residuals. The goal is to fit the weak learner to the errors made by the current ensemble of models.\n",
    "\n",
    "4. **Gradient Descent Optimization:**\n",
    "   - The weak learner is trained to minimize the residual errors by finding the negative gradient of the loss function with respect to the current predictions.\n",
    "   - The learning rate (a hyperparameter) controls the step size in the direction of the negative gradient during optimization.\n",
    "\n",
    "5. **Update Ensemble:**\n",
    "   - The predictions of the weak learner are scaled by a factor (learning rate) and added to the ensemble.\n",
    "   - The ensemble now includes the new weak learner, and its predictions are combined with the predictions from the previous weak learners.\n",
    "\n",
    "6. **Iteration:**\n",
    "   - Steps 2-5 are repeated for a predefined number of iterations or until a convergence criterion is met.\n",
    "   - In each iteration, a new weak learner is added to the ensemble, and the model is refined to reduce the errors of the combined ensemble.\n",
    "\n",
    "7. **Final Ensemble:**\n",
    "   - The final prediction is the sum of the predictions from all the weak learners in the ensemble.\n",
    "   - The combination of weak learners, each addressing a different aspect of the data, leads to a strong predictive model.\n",
    "\n",
    "The process of sequentially adding weak learners and updating the predictions based on the negative gradient is what gives the algorithm its name, \"Gradient Boosting.\" The ensemble gradually adapts to the complexity of the data, capturing intricate patterns and relationships to improve predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b03fecb-7283-4b52-95ea-975619c7f240",
   "metadata": {},
   "source": [
    "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef60fa1-e647-4659-83e3-85844d90bb45",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key mathematical concepts and steps that drive the algorithm's learning process. Here are the steps involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function:**\n",
    "   - Start with a loss function that measures the difference between the model's predictions and the actual target values. For regression problems, the mean squared error (MSE) is commonly used.\n",
    "\n",
    "2. **Initial Prediction:**\n",
    "   - Initialize the model with a simple prediction, often the mean or median of the target variable for regression, or a class distribution for classification.\n",
    "\n",
    "3. **Residual Calculation:**\n",
    "   - Compute the residuals by subtracting the initial prediction from the actual target values. These residuals represent the errors made by the current model.\n",
    "\n",
    "4. **Sequential Weak Learner Training:**\n",
    "   - Train a weak learner (usually a decision tree) on the residuals. The goal is to fit the weak learner to the errors made by the current ensemble.\n",
    "\n",
    "5. **Negative Gradient:**\n",
    "   - Calculate the negative gradient of the loss function with respect to the current predictions. This indicates the direction and magnitude of the adjustment needed to minimize the loss.\n",
    "\n",
    "6. **Scaling by Learning Rate:**\n",
    "   - Scale the predictions of the weak learner by a factor known as the learning rate. This controls the step size during the gradient descent optimization process.\n",
    "\n",
    "7. **Update Ensemble:**\n",
    "   - Add the scaled predictions of the weak learner to the current ensemble of models. This updates the overall prediction by incorporating the information from the new weak learner.\n",
    "\n",
    "8. **Repeat:**\n",
    "   - Repeat steps 3-7 for a predefined number of iterations or until a convergence criterion is met. In each iteration, a new weak learner is trained to capture the remaining errors.\n",
    "\n",
    "9. **Final Prediction:**\n",
    "   - The final prediction is the sum of the predictions from all the weak learners, each scaled by the learning rate.\n",
    "\n",
    "10. **Regularization:**\n",
    "    - Optionally, introduce regularization techniques to prevent overfitting, such as limiting the depth of the weak learners or adding a regularization term to the loss function.\n",
    "\n",
    "11. **Evaluation:**\n",
    "    - Evaluate the performance of the final ensemble on a validation or test dataset using appropriate metrics (e.g., mean squared error for regression, accuracy for classification).\n",
    "\n",
    "Understanding the mathematical intuition involves grasping how the negative gradient guides the optimization process, how the weak learners are combined to reduce errors, and how the algorithm adapts to the data's complexity over iterations. It also involves recognizing the role of hyperparameters like the learning rate in controlling the update of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff47e6c-c93a-4109-8c26-3d86e34ac5e8",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53399c55-546c-4d27-b033-20164645bb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
